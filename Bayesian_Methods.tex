\documentclass[11pt, a4paper]{report}

%----------------------------------------------------------------------------------------
% PACKAGES
%----------------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Math Packages
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{physics} % Note: physics can sometimes clash with other math packages
\usepackage{newtxmath} % Usually best loaded after other math/font packages

% Fonts
\usepackage{libertine}
\usepackage{microtype}

\usepackage{verbatim}
\usepackage{zref-clever}

% Graphics and Formatting
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{authblk}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% Header and Footer Customization (ADDED THIS)
\usepackage{fancyhdr}

% Custom Local Packages
% \usepackage{Notation} % <--- COMMENTED OUT: Only uncomment if you have Notation.sty file

% Utilities
%\usepackage{todonotes}
% \usepackage[capitalise]{cleveref}
\usepackage{natbib}
\usepackage[pdfencoding=auto]{hyperref} % Load last usually %----------------------------------------------------------------------------------------
% THEOREMS
%----------------------------------------------------------------------------------------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

% --------------------------------------------------
% Notation shortcuts
% --------------------------------------------------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\R}{\mathbb{R}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{\textbf{ Bayesian Inference}\\
    \large Mathematical Proofs and Worked Examples}
\author{Donnie \\ PhD Student, Trinity College Dublin}
\date{}
\maketitle

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Philosophy and Scope}

This document provides a rigorous mathematical foundation for the
\emph{Interactive Bayesian Inference Explorer}, an educational Shiny
application designed to make Bayesian inference explicit, inspectable, and
structurally transparent.

The primary objective is not computational efficiency but conceptual clarity.
In particular, the document emphasises:

\begin{itemize}
    \item Bayes' rule as an identity of probability measures,
    \item conjugacy as kernel preservation,
    \item priors as pseudo-data and regularisers,
    \item the relationship between exact Bayesian inference and approximation.
\end{itemize}

This monograph is self-contained and assumes familiarity with probability
theory, calculus, and linear algebra at graduate level.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probability Foundations}

\subsection{Probability Spaces and Random Elements}

\begin{definition}
A probability space is a triple $(\Omega,\mathcal{F},\mathbb{P})$, where
$\Omega$ is a sample space, $\mathcal{F}$ a $\sigma$-algebra, and
$\mathbb{P}$ a probability measure.
\end{definition}

A parameter $\theta$ and data $Y$ are modelled as random elements defined on
the same probability space.

\begin{remark}
Bayesian inference treats unknown parameters as random variables.
This is a modelling choice, not a claim about physical randomness.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Models as Joint Distributions}

\subsection{Joint and Conditional Structure}

Bayesian modelling begins with the specification of a joint distribution
$$
 p(\theta,y) = \underbrace{p(y\mid\theta)}_{\text{likelihood}}\,\underbrace{\pi(\theta)}_{\text{prior}}.
$$

The posterior distribution follows from conditioning:
$$
 \pi(\theta\mid y)
 =
 \frac{\underbrace{p(y\mid\theta)}_{\text{likelihood}}\,\underbrace{\pi(\theta)}_{\text{prior}}}{\underbrace{p(y)}_{\text{evidence / normaliser}}}.
$$

\begin{definition}[Evidence]
The marginal likelihood (evidence) is
$$
 p(y)
 =
 \underbrace{\int \underbrace{p(y\mid\theta)}_{\text{likelihood}}\,\underbrace{\pi(\theta)}_{\text{prior}}\,d\theta}_{\text{prior predictive / marginal likelihood}}.
$$
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayes' Theorem: Measure-Theoretic Form}

\begin{theorem}[Bayes' Rule]
Let $(\theta,Y)$ admit a joint probability distribution. Then there exists a
regular conditional distribution $\mathbb{P}(\theta\in\cdot\mid Y)$.
\end{theorem}

\begin{proof}
We sketch the construction in a way that makes explicit what is being claimed.

Let $(\Omega,\mathcal{F},\mathbb{P})$ be the underlying probability space and
assume $\theta$ and $Y$ are measurable maps into measurable spaces
$(\Theta,\mathcal{B}_\Theta)$ and $(\mathcal{Y},\mathcal{B}_\mathcal{Y})$.
Denote by $\mathbb{P}_{\theta,Y}$ the joint law on
$(\Theta\times\mathcal{Y},\mathcal{B}_\Theta\otimes\mathcal{B}_\mathcal{Y})$.

For any measurable set $A\in\mathcal{B}_\Theta$, define a finite measure
$\nu_A$ on $\mathcal{Y}$ by
$$
\nu_A(B) := \mathbb{P}(\theta\in A,\ Y\in B),\qquad B\in\mathcal{B}_\mathcal{Y}.
$$
Also let $\mu$ denote the marginal law of $Y$:
$$
\mu(B) := \mathbb{P}(Y\in B),\qquad B\in\mathcal{B}_\mathcal{Y}.
$$
Clearly $\nu_A\ll \mu$ (absolute continuity) because $\mu(B)=0$ implies
$\nu_A(B)\le \mathbb{P}(Y\in B)=0$.
Therefore, by the Radon--Nikodym theorem, there exists a measurable function
$g_A(y)$ such that
$$
\nu_A(B)=\int_B g_A(y)\,\mu(dy),\qquad\text{for all }B\in\mathcal{B}_\mathcal{Y}.
$$

Define
$$
\mathbb{P}(\theta\in A\mid Y=y) := g_A(y).
$$
One can verify:
\begin{itemize}
    \item for fixed $A$, $y\mapsto \mathbb{P}(\theta\in A\mid Y=y)$ is measurable,
    \item for $\mu$-almost every $y$, the set function $A\mapsto \mathbb{P}(\theta\in A\mid Y=y)$ is a probability measure on $\Theta$,
    \item and the defining consistency property holds:
    $$
    \mathbb{P}(\theta\in A,\ Y\in B)
    = \int_B \mathbb{P}(\theta\in A\mid Y=y)\,\mu(dy).
    $$
\end{itemize}
This $\mathbb{P}(\theta\in\cdot\mid Y=y)$ is the regular conditional distribution.
When densities exist, this construction reduces to the familiar formula
$\pi(\theta\mid y)\propto p(y\mid\theta)\pi(\theta)$.
\end{proof}

\begin{remark}
The density-based Bayes' rule is a special case of this theorem.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Decision Theory}

\begin{definition}
Let $\ell(a,\theta)$ be a loss function. The Bayes action is
$$
a^*(y) = \arg\min_a \E[\ell(a,\theta)\mid y].
$$
\end{definition}

\begin{proposition}
Under squared error loss, the Bayes action is the posterior mean.
\end{proposition}

\begin{proof}
Consider
$$
\E[(a-\theta)^2\mid y]
=
 \underbrace{(a-\E[\theta\mid y])^2}_{\text{error from choosing }a\text{ instead of }\E[\theta\mid y]}
 +
 \underbrace{\Var(\theta\mid y)}_{\text{irreducible posterior uncertainty}}.
$$
Minimisation occurs at $a=\E[\theta\mid y]$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MAP Estimation and Penalised Likelihood}

The MAP estimator satisfies
$$
\theta_{\mathrm{MAP}} = \arg\max_\theta
\left\{
 \underbrace{\log p(y\mid\theta)}_{\text{data fit (log-likelihood)}}
 +
 \underbrace{\log\pi(\theta)}_{\text{regulariser (log-prior density)}}
\right\}.
$$

\begin{remark}
This reveals MAP estimation as penalised likelihood.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kullback--Leibler Characterisation}

\begin{theorem}
The posterior distribution minimises KL divergence:
$$
\pi(\theta\mid y)
=
\arg\min_q \KL(q(\theta)\|\pi(\theta\mid y)).
$$
\end{theorem}

\begin{proof}
We give a complete derivation, which is the foundation of variational Bayes.

Assume $q$ is absolutely continuous with respect to $\pi(\theta\mid y)$.
By definition,
$$
\KL\big(q(\theta)\|\pi(\theta\mid y)\big)
=\int q(\theta)\log\frac{q(\theta)}{\pi(\theta\mid y)}\,d\theta.
$$
Using Bayes' rule $\pi(\theta\mid y)=p(y,\theta)/p(y)$, we obtain
$$
\log\frac{q(\theta)}{\pi(\theta\mid y)}
=\log q(\theta) - \log p(y,\theta) + \log p(y).
$$
Integrating with respect to $q$ yields
$$
\KL\big(q\|\pi(\theta\mid y)\big)
= \E_q[\log q(\theta)] - \E_q[\log p(y,\theta)] + \log p(y).
$$
Rearranging gives the fundamental decomposition
$$
 \underbrace{\log p(y)}_{\text{log evidence (constant in }q\text{)}}
 =
 \underbrace{\E_q[\log p(y,\theta)] - \E_q[\log q(\theta)]}_{\text{ELBO }\mathcal{L}(q)}
 +
 \underbrace{\KL\big(q\|\pi(\theta\mid y)\big)}_{\ge 0\ \text{gap to posterior}}.
$$
Define the evidence lower bound (ELBO)
$$
 \mathcal{L}(q)
 :=
 \underbrace{\E_q[\log p(y,\theta)]}_{\text{expected log joint}}
 -
 \underbrace{\E_q[\log q(\theta)]}_{\text{negative entropy of }q}.
$$
Since $\KL(\cdot\|\cdot)\ge 0$ with equality iff $q=\pi(\theta\mid y)$ almost
everywhere, it follows that $\mathcal{L}(q)\le \log p(y)$, and the unique
minimiser of $\KL(q\|\pi(\theta\mid y))$ is the posterior itself.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exchangeability and de Finetti}

\begin{theorem}[de Finetti]
An infinite exchangeable Bernoulli sequence satisfies
$$
\Pr(Y_{1:n}=y_{1:n})
=
 \int \underbrace{\prod_{i=1}^n \theta^{y_i}(1-\theta)^{1-y_i}}_{\text{i.i.d. Bernoulli likelihood given }\theta}\, \underbrace{d\Pi(\theta)}_{\text{mixing measure / prior on }\theta}.
$$
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponential Families and Conjugacy}

\subsection{General Form}

An exponential family likelihood takes the form
$$
p(y\mid\theta)
=
 \underbrace{h(y)}_{\text{base measure}}\exp\left(
 \underbrace{\eta(\theta)^\top}_{\text{natural parameter}}\underbrace{T(y)}_{\text{sufficient statistic}}
 -
 \underbrace{A(\theta)}_{\text{log-partition}}\right).
$$

\begin{proposition}
Conjugate priors preserve the functional form of the posterior.
\end{proposition}

We now formalise this statement and derive the general update.

\subsection{Canonical conjugate prior and parameter update}

Assume the likelihood admits the (possibly vector-valued) exponential family
representation
$$
p(y\mid\theta)
=h(y)\exp\left(\eta(\theta)^\top T(y)-A(\theta)\right).
$$
For i.i.d. observations $y_{1:n}$, the likelihood factorises and the sufficient
statistics add:
$$
p(y_{1:n}\mid\theta)
\propto \exp\left(\eta(\theta)^\top \sum_{i=1}^n T(y_i) - nA(\theta)\right).
$$

The canonical conjugate prior has the form
$$
\pi(\theta\mid \lambda,\nu)
\propto
 \exp\left(
 \underbrace{\eta(\theta)^\top\lambda}_{\text{pseudo-statistic term}}
 -
 \underbrace{\nu A(\theta)}_{\text{strength / pseudo-sample-size term}}\right),
$$
for hyperparameters $(\lambda,\nu)$ in a set that ensures normalisability.

\begin{theorem}[General conjugate update]
Under the exponential family likelihood above and the canonical conjugate prior,
the posterior remains in the same conjugate family with updated hyperparameters
$$
 \lambda_n
 =
 \underbrace{\lambda}_{\text{prior pseudo-statistic}}
 +
 \underbrace{\sum_{i=1}^n T(y_i)}_{\text{data sufficient statistic}},
\qquad
 \nu_n
 =
 \underbrace{\nu}_{\text{prior strength}}
 +
 \underbrace{n}_{\text{sample size}}.
$$
\end{theorem}

\begin{proof}
By Bayes' rule,
$$
\pi(\theta\mid y_{1:n})
\propto
p(y_{1:n}\mid\theta)\,\pi(\theta\mid\lambda,\nu).
$$
Dropping terms that do not depend on $\theta$ and substituting the exponential
family likelihood and conjugate prior kernels gives
\begin{align*}
\pi(\theta\mid y_{1:n})
&\propto
\exp\left(\eta(\theta)^\top \sum_{i=1}^n T(y_i) - nA(\theta)\right)
\exp\left(\eta(\theta)^\top\lambda - \nu A(\theta)\right)\\
&=
\exp\left(\eta(\theta)^\top\bigg(\lambda+\sum_{i=1}^n T(y_i)\bigg) - (\nu+n)A(\theta)\right).
\end{align*}
This is exactly the conjugate prior kernel with hyperparameters
$(\lambda_n,\nu_n)=(\lambda+\sum_i T(y_i),\nu+n)$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Binomial Model}

\subsection{Likelihood}

Let $X\mid\theta\sim\mathrm{Binomial}(n,\theta)$:
$$
p(x\mid\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x}.
$$

\subsection{Beta Prior}

Let $\theta\sim\mathrm{Beta}(\alpha,\beta)$.

\begin{theorem}
The posterior is
$$
 \theta\mid x\sim\mathrm{Beta}(\underbrace{\alpha+x}_{\text{prior successes }\alpha\ \text{+ data successes }x},\underbrace{\beta+n-x}_{\text{prior failures }\beta\ \text{+ data failures }(n-x)}).
$$
\end{theorem}

\begin{proof}
We write the derivation in full.

The Binomial likelihood is
$$
p(x\mid\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x}.
$$
The Beta prior density is
$$
\pi(\theta)=\frac{1}{\mathrm{B}(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1},
\qquad 0<\theta<1,
$$
where the Beta function is $\mathrm{B}(\alpha,\beta)=\Gamma(\alpha)\Gamma(\beta)/\Gamma(\alpha+\beta)$.
Then the posterior kernel is
\begin{align*}
\pi(\theta\mid x)
&\propto p(x\mid\theta)\pi(\theta)\\
&\propto \theta^x(1-\theta)^{n-x}\,\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
&=\theta^{\alpha+x-1}(1-\theta)^{\beta+n-x-1}.
\end{align*}
This is the kernel of a Beta distribution with parameters
$(\alpha+x,\beta+n-x)$.
\end{proof}

\subsection{Posterior summaries (mean, variance, MAP)}

If $\theta\mid x\sim\mathrm{Beta}(\alpha',\beta')$ with
$\alpha'=\alpha+x$ and $\beta'=\beta+n-x$, then
$$
\E[\theta\mid x]
=
\underbrace{\frac{\alpha'}{\alpha'+\beta'}}_{\text{posterior mean}}\,=\,\underbrace{\frac{\alpha+x}{\alpha+\beta+n}}_{\text{successes / total pseudo-count}},
\qquad
\Var(\theta\mid x)
=
\underbrace{\frac{\alpha'\beta'}{(\alpha'+\beta')^2(\alpha'+\beta'+1)}}_{\text{posterior variance}}.
$$
If $\alpha'>1$ and $\beta'>1$, the posterior mode (MAP) is
$$
\theta_{\mathrm{MAP}}
=
\underbrace{\frac{\alpha'-1}{\alpha'+\beta'-2}}_{\text{posterior mode (interior case)}}.
$$

\subsection{Worked Example}

Suppose $n=20$, $x=14$, $\alpha=\beta=1$.
Then
$$
\theta\mid x\sim\mathrm{Beta}(\underbrace{15}_{\alpha+x},\underbrace{7}_{\beta+n-x}).
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Posterior Predictive: Beta--Binomial}

Let $\tilde X\mid\theta\sim\mathrm{Binomial}(m,\theta)$.

\begin{theorem}[Posterior predictive distribution]
Under the Beta--Binomial model with $\theta\mid x\sim\mathrm{Beta}(\alpha',\beta')$,
the posterior predictive distribution for $\tilde X\mid\theta\sim\mathrm{Binomial}(m,\theta)$ is
$$
\tilde X\mid x\sim\mathrm{BetaBinomial}(\underbrace{m}_{\text{future trials}},\underbrace{\alpha'}_{\text{posterior success pseudo-count}},\underbrace{\beta'}_{\text{posterior failure pseudo-count}}).
$$
\end{theorem}

\begin{proof}
By definition,
\begin{align*}
p(\tilde x\mid x)
&=\int_0^1 p(\tilde x\mid\theta)\,\pi(\theta\mid x)\,d\theta\\
&=\int_0^1 \binom{m}{\tilde x}\theta^{\tilde x}(1-\theta)^{m-\tilde x}
\frac{1}{\mathrm{B}(\alpha',\beta')}
\theta^{\alpha'-1}(1-\theta)^{\beta'-1}\,d\theta\\
&=\binom{m}{\tilde x}\frac{1}{\mathrm{B}(\alpha',\beta')}
\int_0^1 \theta^{\alpha'+\tilde x-1}(1-\theta)^{\beta'+m-\tilde x-1}\,d\theta.
\end{align*}
The integral is a Beta function:
$$
\int_0^1 \theta^{\alpha'+\tilde x-1}(1-\theta)^{\beta'+m-\tilde x-1}\,d\theta
=\mathrm{B}(\alpha'+\tilde x,\beta'+m-\tilde x).
$$
Hence
$$
p(\tilde x\mid x)
=\binom{m}{\tilde x}\frac{\mathrm{B}(\alpha'+\tilde x,\beta'+m-\tilde x)}{\mathrm{B}(\alpha',\beta')},
$$
which is the Beta--Binomial pmf.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson Model}

\subsection{Likelihood}

Let $Y_i\mid\lambda\sim\mathrm{Poisson}(\lambda)$.

For i.i.d. data $y_{1:n}$, the joint likelihood is
\begin{align*}
p(y_{1:n}\mid\lambda)
&=\prod_{i=1}^n e^{-\lambda}\frac{\lambda^{y_i}}{y_i!}
=e^{-n\lambda}\lambda^{\sum_{i=1}^n y_i}\prod_{i=1}^n \frac{1}{y_i!}.
\end{align*}
Let $S=\sum_{i=1}^n y_i$. Then the likelihood kernel is
$$
p(y_{1:n}\mid\lambda)\propto
\underbrace{\lambda^{S}}_{\text{data term: total count }S}\,\underbrace{e^{-n\lambda}}_{\text{exposure term: }n}.
$$

\subsection{Gamma Prior}

Let $\lambda\sim\mathrm{Gamma}(\alpha,\beta)$.

\begin{theorem}
$$
\lambda\mid y\sim\mathrm{Gamma}(\underbrace{\alpha+S}_{\text{shape: prior }\alpha\text{ + total count }S},\underbrace{\beta+n}_{\text{rate: prior }\beta\text{ + exposure }n}).
$$
\end{theorem}

\begin{proof}
The Gamma prior density (shape--rate) is
$$
\pi(\lambda)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda},\qquad \lambda>0.
$$
Multiplying the likelihood kernel and the prior kernel yields
\begin{align*}
\pi(\lambda\mid y)
&\propto \lambda^{S}e^{-n\lambda}\,\lambda^{\alpha-1}e^{-\beta\lambda}
=\lambda^{\alpha+S-1}e^{-(\beta+n)\lambda},
\end{align*}
which is the kernel of a $\mathrm{Gamma}(\alpha+S,\beta+n)$ distribution.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Posterior Predictive: Negative Binomial}

\subsection{Gamma--Poisson mixture derivation}

Let $\tilde Y\mid\lambda\sim\mathrm{Poisson}(\lambda)$ be a future observation.
The posterior predictive is
$$
p(\tilde y\mid y)
=
\int_0^\infty \underbrace{p(\tilde y\mid\lambda)}_{\text{Poisson sampling model}}\,\underbrace{\pi(\lambda\mid y)}_{\text{Gamma posterior}}\,d\lambda.
$$
With $\pi(\lambda\mid y)=\mathrm{Gamma}(\alpha',\beta')$ where
$\alpha'=\alpha+S$ and $\beta'=\beta+n$, we compute
\begin{align*}
p(\tilde y\mid y)
&=\int_0^\infty \left(e^{-\lambda}\frac{\lambda^{\tilde y}}{\tilde y!}\right)
\left(\frac{\beta'^{\alpha'}}{\Gamma(\alpha')}\lambda^{\alpha'-1}e^{-\beta'\lambda}\right)d\lambda\\
&=\frac{\beta'^{\alpha'}}{\tilde y!\,\Gamma(\alpha')}
\int_0^\infty \lambda^{\alpha'+\tilde y-1}e^{-(\beta'+1)\lambda}\,d\lambda.
\end{align*}
The remaining integral is a Gamma integral:
$$
\int_0^\infty \lambda^{\alpha'+\tilde y-1}e^{-(\beta'+1)\lambda}\,d\lambda
=\frac{\Gamma(\alpha'+\tilde y)}{(\beta'+1)^{\alpha'+\tilde y}}.
$$
Therefore
$$
p(\tilde y\mid y)
=\frac{\Gamma(\alpha'+\tilde y)}{\Gamma(\alpha')\,\tilde y!}
\left(\frac{\beta'}{\beta'+1}\right)^{\alpha'}
\left(\frac{1}{\beta'+1}\right)^{\tilde y}.
$$
We can view this as
$$
p(\tilde y\mid y)
=
\underbrace{\frac{\Gamma(\alpha'+\tilde y)}{\Gamma(\alpha')\,\tilde y!}}_{\text{combinatorial term}}
\underbrace{\left(\frac{\beta'}{\beta'+1}\right)^{\alpha'}}_{\text{success prob}^{\alpha'}}
\underbrace{\left(\frac{1}{\beta'+1}\right)^{\tilde y}}_{\text{failure prob}^{\tilde y}}.
$$
This is a Negative Binomial pmf under a common parameterisation.

\begin{remark}
The derivation shows that a Negative Binomial distribution arises as a
Poisson distribution with a Gamma-mixed rate. This is the canonical Bayesian
explanation of count overdispersion.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normal Models}

\subsection{Known Variance}

\begin{theorem}
If $Y_i\mid\mu\sim\mathcal{N}(\mu,\sigma^2)$ and
$\mu\sim\mathcal{N}(\mu_0,\tau_0^2)$, then
$$
\mu\mid y\sim\mathcal{N}(\underbrace{\mu_n}_{\text{posterior mean}},\underbrace{\tau_n^2}_{\text{posterior variance}}).
$$
\end{theorem}

\begin{proof}
Assume $\sigma^2$ is known and $y_{1:n}$ are observed.
The likelihood is
$$
p(y_{1:n}\mid\mu)\propto
\exp\left(-\underbrace{\frac{1}{2\sigma^2}}_{\text{precision factor}}\underbrace{\sum_{i=1}^n (y_i-\mu)^2}_{\text{sum of squares}}\right).
$$
Using the identity
$$
\sum_{i=1}^n (y_i-\mu)^2
=\sum_{i=1}^n (y_i-\bar y)^2 + n(\bar y-\mu)^2,
\qquad \bar y=\frac{1}{n}\sum_{i=1}^n y_i,
$$
the likelihood kernel in $\mu$ becomes
$$
p(y_{1:n}\mid\mu)\propto \exp\left(-\frac{n}{2\sigma^2}(\mu-\bar y)^2\right).
$$
The prior is
$$
\pi(\mu)\propto \exp\left(-\frac{1}{2\tau_0^2}(\mu-\mu_0)^2\right).
$$
Multiplying likelihood and prior yields
$$
\pi(\mu\mid y)\propto
\exp\left(-\frac{1}{2}\left(
\underbrace{\frac{n}{\sigma^2}(\mu-\bar y)^2}_{\text{data quadratic term}}
+
\underbrace{\frac{1}{\tau_0^2}(\mu-\mu_0)^2}_{\text{prior quadratic term}}
\right)\right).
$$
Completing the square, we obtain a Normal posterior with precision additivity:
$$
\underbrace{\frac{1}{\tau_n^2}}_{\text{posterior precision}}
=
\underbrace{\frac{1}{\tau_0^2}}_{\text{prior precision}}
+
\underbrace{\frac{n}{\sigma^2}}_{\text{data precision}},
\qquad
\mu_n
=
\tau_n^2\left(
\underbrace{\frac{\mu_0}{\tau_0^2}}_{\text{prior precision-weighted mean}}
+
\underbrace{\frac{n\bar y}{\sigma^2}}_{\text{data precision-weighted mean}}
\right).
$$
\end{proof}

\subsection{Posterior predictive distribution}

For a new observation $\tilde Y\mid\mu\sim\mathcal{N}(\mu,\sigma^2)$,
the posterior predictive is Normal:
$$
\tilde Y\mid y \sim \mathcal{N}\left(\underbrace{\mu_n}_{\text{predictive mean}},\underbrace{\sigma^2}_{\text{sampling noise}}+\underbrace{\tau_n^2}_{\text{parameter uncertainty}}\right).
$$
This provides analytic uncertainty quantification for predictions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normal--Inverse-Gamma Model (Unknown Mean and Variance)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Process Regression}

Gaussian Processes (GPs) provide a nonparametric Bayesian prior over functions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussian Process Prior}

A Gaussian Process is defined as
$$
f \sim \underbrace{\mathrm{GP}(m(\cdot), k(\cdot,\cdot))}_{\text{prior over functions}},
$$
meaning that for any finite set $\{x_1,\dots,x_n\}$,
$$
(f(x_1),\dots,f(x_n))^\top
\sim
\mathcal{N}(\underbrace{m}_{\text{mean vector}}, \underbrace{K}_{\text{kernel (covariance) matrix}}),
$$
where $K_{ij} = k(x_i,x_j)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regression Model}

Assume
$$
y_i = f(x_i) + \epsilon_i,
\qquad
\epsilon_i \sim \underbrace{\mathcal{N}(0,\sigma^2)}_{\text{i.i.d. Gaussian noise}}.
$$

Let $K = K(X,X)$ be the kernel matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Posterior over Functions}

\begin{theorem}
The posterior distribution of $f_*$ at new inputs $X_*$ is
$$
f_* \mid y \sim \mathcal{N}(\underbrace{m_*}_{\text{posterior mean function}}, \underbrace{\Sigma_*}_{\text{posterior covariance}}),
$$
where
$$
 m_* = \underbrace{K(X_*,X)}_{\text{cross-cov}}\underbrace{(K+\sigma^2 I)^{-1}}_{\text{noisy precision}}\underbrace{y}_{\text{observations}},
$$
$$
 \Sigma_* = \underbrace{K(X_*,X_*)}_{\text{prior variance at }X_*}
 - \underbrace{K(X_*,X)(K+\sigma^2 I)^{-1}K(X,X_*)}_{\text{variance reduction from data}}.
$$
\end{theorem}

\begin{proof}
Apply conditioning rules for multivariate Gaussian distributions.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Posterior Predictive}

The predictive distribution for $\tilde y_*$ is
$$
\tilde y_* \mid y \sim \mathcal{N}\left(\underbrace{m_*}_{\text{predictive mean}},\underbrace{\Sigma_*}_{\text{latent uncertainty}} + \underbrace{\sigma^2 I}_{\text{observation noise}}\right).
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interpretation}

\begin{itemize}
    \item The posterior mean is a kernel-weighted smoother.
    \item The posterior variance reflects distance from observed data.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dirichlet Process Mixture Models}

Dirichlet Process (DP) mixtures provide a flexible Bayesian framework for
density estimation and clustering.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dirichlet Process Prior}

\begin{definition}
A random measure $G$ follows a Dirichlet Process,
$$
G \sim \underbrace{\mathrm{DP}(\alpha, G_0)}_{\text{concentration }\alpha\ \text{and base measure }G_0},
$$
if for any measurable partition $(A_1,\dots,A_k)$,
$$
(G(A_1),\dots,G(A_k)) \sim \mathrm{Dirichlet}(\underbrace{\alpha G_0(A_1)}_{\text{pseudo-count for }A_1},\dots,\underbrace{\alpha G_0(A_k)}_{\text{pseudo-count for }A_k}).
$$
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DP Mixture Model}

Let
$$
\theta_i \mid G \sim \underbrace{G}_{\text{random discrete measure}},
\qquad
y_i \mid \theta_i \sim \underbrace{p(y_i \mid \theta_i)}_{\text{component likelihood}}.
$$

Integrating out $G$ induces clustering among $\theta_i$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Chinese Restaurant Process Representation}

\begin{theorem}
Marginally, the assignment variables $c_i$ follow
$$
\Pr(c_i = k \mid c_{-i})
=
\begin{cases}
\underbrace{\frac{n_k}{n-1+\alpha}}_{\text{proportional to cluster size }n_k}, & \text{existing cluster} \\
\underbrace{\frac{\alpha}{n-1+\alpha}}_{\text{proportional to }\alpha}, & \text{new cluster}.
\end{cases}
$$
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Collapsed Gibbs Sampling}

Integrating out $G$ yields a collapsed Gibbs sampler:

\begin{enumerate}
    \item Remove observation $i$ from its cluster.
    \item Sample new assignment $c_i$ from
    $$
    p(c_i = k \mid y_i, c_{-i})
    \propto
    \begin{cases}
    \underbrace{n_k}_{\text{prior mass}}\,\underbrace{p(y_i \mid y_{-i,k})}_{\text{posterior predictive in cluster}}, & \text{existing cluster} \\
    \underbrace{\alpha}_{\text{new-cluster mass}}\,\underbrace{\int p(y_i \mid \theta)\,dG_0(\theta)}_{\text{prior predictive under }G_0}, & \text{new cluster}.
    \end{cases}
    $$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussian DP Mixture}

For
$$
y_i \mid \theta_i \sim \underbrace{\mathcal{N}(\mu_{c_i},\sigma^2)}_{\text{cluster-specific likelihood}},
\qquad
\mu_k \sim \underbrace{\mathcal{N}(\mu_0,\tau_0^2)}_{\text{base prior for component means}},
$$
all conditional distributions are available in closed form.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interpretation}

\begin{itemize}
    \item The DP induces a random number of clusters.
    \item $\alpha$ controls cluster proliferation.
    \item Posterior inference performs simultaneous density estimation and clustering.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final Perspective}

These three extensions illustrate a unifying principle of Bayesian computation:
\emph{introduce structure (latent variables, infinite-dimensional priors,
kernels) to recover tractability and interpretability}.

They form the conceptual bridge between classical conjugate models and modern
Bayesian machine learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concluding Remarks}

Bayesian inference is a coherent mathematical system in which probability,
decision theory, and computation interact seamlessly.

This monograph demonstrates that the algorithms implemented in the Shiny
application are not ad hoc procedures but direct consequences of probability
theory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

